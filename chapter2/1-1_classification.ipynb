{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像分類のファインチューニング\n",
    "## ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas plotly torch torchvision kaleido nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの作成\n",
    "Resnet50 のモデルを読み込み、最後の全結合層を取り除いて、新しい全結合層を追加する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(pretrained: bool = True, state_dict: dict | None = None):\n",
    "    \"\"\"モデルの取得\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool, optional): 事前学習済みの重みを読み込むか. Defaults to True.\n",
    "        state_dict (dict | None, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # 事前学習済みのResNetモデルをロード\n",
    "    model = models.resnet50(pretrained=pretrained)\n",
    "    # ResNetの最後の全結合層をクラス数に置き換え\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    # デバイスの選択、GPUが使用可能なら使う\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # 損失関数の設定\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # オプティマイザの設定\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行関数を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_random_sampler(dataset, subset_size=1000, random_seed=42):\n",
    "    \"\"\"データセットからランダムにデータを取得するためのSubsetRandomSamplerを作成する\n",
    "\n",
    "    Args:\n",
    "        dataset (_type_): 対象データセット\n",
    "        subset_size (int, optional): ランダムに抽出するデータ数. Defaults to 1000.\n",
    "        random_seed (int, optional): seed値. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # データセットのインデックス配列を作成する\n",
    "    indices = list(range(len(dataset)))\n",
    "    # インデックスをシャッフルする\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    # シャッフルしたインデックスからsubset_size分だけ取得する\n",
    "    subset_indices = indices[:subset_size]\n",
    "    # SubsetRandomSamplerにインデックスを渡すことで、そのインデックスのデータをサンプリングする\n",
    "    return SubsetRandomSampler(subset_indices)\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"1エポック分の学習を行う\n",
    "\n",
    "    Args:\n",
    "        model (_type_):\n",
    "        train_loader (_type_):\n",
    "        criterion (_type_):\n",
    "        optimizer (_type_):\n",
    "        device (_type_):\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # モデルをtrainモードにする\n",
    "    model.train()\n",
    "    # 損失を記録する変数を定義\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # ミニバッチごとにループを回す\n",
    "    for images, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # 勾配を初期化する\n",
    "        optimizer.zero_grad()\n",
    "        # 準伝搬\n",
    "        outputs = model(images)\n",
    "        # 損失関数を計算\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 逆伝搬\n",
    "        loss.backward()\n",
    "        # パラメータ更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # ミニバッチの損失を計算し記録する\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 1エポックあたりの平均損失を計算する\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"1エポック分の検証を行う\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        val_loader (_type_): _description_\n",
    "        criterion (_type_): _description_\n",
    "        device (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # モデルをevalモードにする\n",
    "    model.eval()\n",
    "    # 損失を記録する変数を定義\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_output = []\n",
    "    all_labels = []\n",
    "    # 勾配計算をしないようにする(推論なので)\n",
    "    with torch.no_grad():\n",
    "        # ミニバッチごとにループを回す\n",
    "        for images, labels in tqdm(val_loader, total=len(val_loader)):\n",
    "            # デバイスの指定\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # 準伝搬\n",
    "            outputs = model(images)\n",
    "            all_output.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            # 損失を記録する\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # 1エポックあたりの平均損失を計算する\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    # テストデータの予測結果を取得する\n",
    "    all_output = torch.cat(all_output, dim=0).cpu()\n",
    "    all_labels = torch.cat(all_labels, dim=0).cpu()\n",
    "    return avg_loss, all_output, all_labels\n",
    "\n",
    "\n",
    "def get_cifar10_train_test_loader(\n",
    "    train_samples: int = 1000,\n",
    "    test_samples: int = 1000,\n",
    "    resize: tuple[int, int] = (256, 256),\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    \"\"\"CIFAR-10データセットの学習データと検証データのDataLoaderを作成する\n",
    "\n",
    "    Args:\n",
    "        train_samples (int, optional): _description_. Defaults to 1000.\n",
    "        test_samples (int, optional): _description_. Defaults to 1000.\n",
    "        resize (tuple[int, int], optional): _description_. Defaults to (256, 256).\n",
    "        batch_size (int, optional): _description_. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # 画像を256x156にリサイズして、テンソルに変換する\n",
    "    transform = transforms.Compose([transforms.Resize(resize), transforms.ToTensor()])\n",
    "\n",
    "    # 学習データセットの作成\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    # データセットからランダムにデータを取得する\n",
    "    train_sampler = get_random_sampler(train_dataset, train_samples)\n",
    "    # 学習DataLoaderの作成\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    # 検証データセットの作成\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    # データセットからランダムにデータを取得する\n",
    "    test_sampler = get_random_sampler(test_dataset, test_samples)\n",
    "    # 検証DataLoaderの作成\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    train_samples: int = 1000,\n",
    "    test_samples: int = 1000,\n",
    "    pretrained: bool = True,\n",
    "    num_epochs: int = 50,\n",
    "):\n",
    "    # 事前学習済みのResNetモデルをロード\n",
    "    model, criterion, optimizer = get_model(pretrained=pretrained)\n",
    "\n",
    "    # データをロードする\n",
    "    train_loader, test_loader = get_cifar10_train_test_loader(\n",
    "        train_samples=train_samples, test_samples=test_samples\n",
    "    )\n",
    "\n",
    "    # 結果を書き出すフォルダを作成\n",
    "    result = []\n",
    "    output_dir = Path(\n",
    "        \"output\",\n",
    "        \"classification_cifar10\",\n",
    "        \"pretrained\" if pretrained else \"un_pretrained\",\n",
    "        f\"train_samples_{train_samples}\",\n",
    "    )\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 学習のループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # 学習\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        # 検証\n",
    "        val_loss, predicted_output, true_label = validate_epoch(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        # predicted_output と true_label から accuracy を計算する\n",
    "        _, predicted_class = torch.max(predicted_output, dim=1)\n",
    "        # predicted_class と true_label は同じサイズのテンソルであることを確認\n",
    "        assert predicted_class.size() == true_label.size()\n",
    "        # 予測が正しかった数を計算\n",
    "        correct = (predicted_class == true_label).sum().item()\n",
    "        # テストデータの総数\n",
    "        total = true_label.size(0)\n",
    "\n",
    "        # 正解率の計算\n",
    "        accuracy = correct / total\n",
    "        # 結果の保存\n",
    "        result.append(\n",
    "            {\"train_loss\": train_loss, \"val_loss\": val_loss, \"accuracy\": accuracy}\n",
    "        )\n",
    "\n",
    "        # 結果の表示\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\", result[-1])\n",
    "        # モデルの保存\n",
    "        # torch.save(model.state_dict(), output_dir / f\"check_point_epoch_{epoch}.pt\")\n",
    "\n",
    "    # 結果の保存\n",
    "    df_result = pd.DataFrame(result)\n",
    "    df_result.to_csv(output_dir / \"training_curve.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプル数を変えて学習を回す\n",
    "学習コードができたので、教師データのサンプル数を変えて学習を回してみる。\n",
    "\n",
    "### 実験の設定\n",
    "実験対象のサンプル数と、学習エポック数を設定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 教師データのサンプル数リスト\n",
    "# train_samples_list = [100, 500, 1000, 2000, 3000, 4000, 5000]\n",
    "# 学習エポック数\n",
    "# num_train_epoch = 10\n",
    "\n",
    "# 動作確認用の設定\n",
    "# 教師データのサンプル数リスト\n",
    "train_samples_list = [100, 500]\n",
    "# 学習エポック数\n",
    "num_train_epoch = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# いろいろなサンプル数で学習を実施する\n",
    "for num in train_samples_list:\n",
    "    # ファインチューニング\n",
    "    run(train_samples=num, test_samples=1000, pretrained=True, num_epochs=num_train_epoch)\n",
    "    # ファインチューニングなし\n",
    "    run(train_samples=num, test_samples=1000, pretrained=False, num_epochs=num_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result(pretrained: bool = True, train_samples: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"学習の結果を読み込む\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool, optional): 事前学習の有無. Defaults to True.\n",
    "        train_samples (int, optional): 学習データの数. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # 引数から結果のフォルダを特定\n",
    "    output_dir = Path(\n",
    "        \"output\",\n",
    "        \"classification_cifar10\",\n",
    "        \"pretrained\" if pretrained else \"un_pretrained\",\n",
    "        f\"train_samples_{train_samples}\",\n",
    "    )\n",
    "    # 結果を読み込む\n",
    "    df_result = pd.read_csv(output_dir / \"training_curve.csv\", index_col=0)\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def plot_result(pretrained: bool = True, train_samples: int = 1000):\n",
    "    \"\"\"学習結果をプロットする\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool, optional): 事前学習の有無. Defaults to True.\n",
    "        train_samples (int, optional): 学習データの数. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # 結果の読み込み\n",
    "    df = load_result(pretrained=pretrained, train_samples=train_samples)\n",
    "    # プロット\n",
    "    title = f\"{pretrained=} {train_samples=}\"\n",
    "    fig_loss = px.line(df, y=[\"train_loss\", \"val_loss\"])\n",
    "    fig_loss.update_layout(\n",
    "        title=f\"Loss {title}\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        yaxis_range=[0, 5],\n",
    "    )\n",
    "    os.makedirs(\"output/classification_cifar10/figs\", exist_ok=True)\n",
    "    fig_loss.write_image(\n",
    "        f\"output/classification_cifar10/figs/loss_{title.replace(' ', '_')}.png\"\n",
    "    )\n",
    "    fig_acc = px.line(df, y=[\"accuracy\"])\n",
    "    fig_acc.update_layout(\n",
    "        title=f\"Accuracy {title}\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Test Accuracy\",\n",
    "        yaxis_range=[0, 1],\n",
    "    )\n",
    "    fig_acc.write_image(\n",
    "        f\"output/classification_cifar10/figs/acc_{title.replace(' ', '_')}.png\"\n",
    "    )\n",
    "\n",
    "    return fig_loss, fig_acc\n",
    "\n",
    "\n",
    "def plot_mix_result(train_samples: int = 1000):\n",
    "    \"\"\"事前学習ありなしを比較する\n",
    "\n",
    "    Args:\n",
    "        train_samples (int, optional): 学習データの数. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # 事前学習あり\n",
    "    df_fine = load_result(pretrained=True, train_samples=train_samples)\n",
    "    # 事前学習なし\n",
    "    df_no_fine = load_result(pretrained=False, train_samples=train_samples)\n",
    "\n",
    "    # プロット\n",
    "    fig = go.Figure()\n",
    "    fig.add_scatter(x=df_fine.index, y=df_fine[\"accuracy\"], name=\"fine tuning accuracy\")\n",
    "    fig.add_scatter(\n",
    "        x=df_fine.index, y=df_no_fine[\"accuracy\"], name=\"no fine tuning accuracy\"\n",
    "    )\n",
    "    title = f\"{train_samples=}\"\n",
    "    fig.update_layout(\n",
    "        title=f\"Accuracy {title}\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        yaxis_range=[0, 1],\n",
    "    )\n",
    "    fig.write_image(f\"output/classification_cifar10/figs/acc_fine_{title}.png\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各学習結果について、プロットを作成する\n",
    "for samples in train_samples_list:\n",
    "    plot_result(pretrained=True, train_samples=samples)\n",
    "    plot_result(pretrained=False, train_samples=samples)\n",
    "    plot_mix_result(train_samples=samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習結果をノートブックで確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(pretrained=True, train_samples=train_samples_list[0])[0]\n",
    "plot_result(pretrained=False, train_samples=train_samples_list[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 教師データサンプル数と精度の変化を可視化する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ量変更実験の可視化\n",
    "pretrain_max_list = []\n",
    "no_pretrain_max_list = []\n",
    "for samples in train_samples_list:\n",
    "    pretrain_max_list.append(\n",
    "        load_result(pretrained=True, train_samples=samples)[\"accuracy\"].max()\n",
    "    )\n",
    "    no_pretrain_max_list.append(\n",
    "        load_result(pretrained=False, train_samples=samples)[\"accuracy\"].max()\n",
    "    )\n",
    "df_max = pd.DataFrame(\n",
    "    {\n",
    "        \"pretrain_max\": pretrain_max_list,\n",
    "        \"no_pretrain_max\": no_pretrain_max_list,\n",
    "        \"train_samples\": train_samples_list,\n",
    "    },\n",
    "    index=range(7),\n",
    ")\n",
    "fig = px.scatter(df_max, y=[\"pretrain_max\", \"no_pretrain_max\"], x=\"train_samples\")\n",
    "fig.update_layout(\n",
    "    title=\"学習サンプルサイズと精度\",\n",
    "    xaxis_title=\"Train Samples\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    yaxis_range=[0, 1],\n",
    ")\n",
    "fig.write_image(\"output/classification_cifar10/figs/acc_max.png\")\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
