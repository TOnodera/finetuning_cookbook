{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 距離学習のファインチューニング\n",
    "## ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas plotly torch torchvision scikit-learn plotly tqdm kaleido nbformat pytorch_metric_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from pytorch_metric_learning import distances, losses, regularizers\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習コードの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sampler(dataset, subset_size=1000, random_seed=42):\n",
    "    \"\"\"データセットからランダムにデータを取得するためのSubsetRandomSamplerを作成する\n",
    "\n",
    "    Args:\n",
    "        dataset (_type_): 対象データセット\n",
    "        subset_size (int, optional): ランダムに抽出するデータ数. Defaults to 1000.\n",
    "        random_seed (int, optional): seed値. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # データセットのインデックス配列を作成する\n",
    "    indices = list(range(len(dataset)))\n",
    "    # インデックスをシャッフルする\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    # シャッフルしたインデックスからsubset_size分だけ取得する\n",
    "    subset_indices = indices[:subset_size]\n",
    "    # SubsetRandomSamplerにインデックスを渡すことで、そのインデックスのデータをサンプリングする\n",
    "    return SubsetRandomSampler(subset_indices)\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"1エポック分の学習を行う\n",
    "\n",
    "    Args:\n",
    "        model (_type_):\n",
    "        train_loader (_type_):\n",
    "        criterion (_type_):\n",
    "        optimizer (_type_):\n",
    "        device (_type_):\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # モデルをtrainモードにする\n",
    "    model.train()\n",
    "    # 損失を記録する変数を定義\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # ミニバッチごとにループを回す\n",
    "    for images, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # 勾配を初期化する\n",
    "        optimizer.zero_grad()\n",
    "        # 準伝搬\n",
    "        outputs = model(images)\n",
    "        # 損失関数を計算\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 逆伝搬\n",
    "        loss.backward()\n",
    "        # パラメータ更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # ミニバッチの損失を計算し記録する\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 1エポックあたりの平均損失を計算する\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"1エポック分の検証を行う\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        val_loader (_type_): _description_\n",
    "        criterion (_type_): _description_\n",
    "        device (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # モデルをevalモードにする\n",
    "    model.eval()\n",
    "    # 損失を記録する変数を定義\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_output = []\n",
    "    all_labels = []\n",
    "    # 勾配計算をしないようにする(推論なので)\n",
    "    with torch.no_grad():\n",
    "        # ミニバッチごとにループを回す\n",
    "        for images, labels in tqdm(val_loader, total=len(val_loader)):\n",
    "            # デバイスの指定\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # 準伝搬\n",
    "            outputs = model(images)\n",
    "            all_output.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            # 損失を記録する\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # 1エポックあたりの平均損失を計算する\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    # テストデータの予測結果を取得する\n",
    "    all_output = torch.cat(all_output, dim=0).cpu()\n",
    "    all_labels = torch.cat(all_labels, dim=0).cpu()\n",
    "    return avg_loss, all_output, all_labels\n",
    "\n",
    "\n",
    "def get_cifar10_train_test_loader(\n",
    "    train_samples: int = 1000,\n",
    "    test_samples: int = 1000,\n",
    "    resize: tuple[int, int] = (256, 256),\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    \"\"\"CIFAR-10データセットの学習データと検証データのDataLoaderを作成する\n",
    "\n",
    "    Args:\n",
    "        train_samples (int, optional): _description_. Defaults to 1000.\n",
    "        test_samples (int, optional): _description_. Defaults to 1000.\n",
    "        resize (tuple[int, int], optional): _description_. Defaults to (256, 256).\n",
    "        batch_size (int, optional): _description_. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # 画像を256x156にリサイズして、テンソルに変換する\n",
    "    transform = transforms.Compose([transforms.Resize(resize), transforms.ToTensor()])\n",
    "\n",
    "    # 学習データセットの作成\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    # データセットからランダムにデータを取得する\n",
    "    train_sampler = get_random_sampler(train_dataset, train_samples)\n",
    "    # 学習DataLoaderの作成\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    # 検証データセットの作成\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    # データセットからランダムにデータを取得する\n",
    "    test_sampler = get_random_sampler(test_dataset, test_samples)\n",
    "    # 検証DataLoaderの作成\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのダウンロード\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データをロードする\n",
    "train_loader, test_loader = get_cifar10_train_test_loader(\n",
    "    train_samples=5000, test_samples=1000\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(pretrained: bool = True, state_dict: dict | None = None):\n",
    "    # 距離学習\n",
    "    # 事前学習済みのResNetモデルをロード\n",
    "    model = models.resnet50(pretrained=pretrained)\n",
    "    # ResNetの最後の全結合層をembedding数に置き換え\n",
    "    model.fc = nn.Linear(model.fc.in_features, 128)\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    # デバイスの設定\n",
    "    model.to(device)\n",
    "    # ArcFace lossの設定\n",
    "    # コサイン類似度を使う\n",
    "    distance = distances.CosineSimilarity()\n",
    "    regularizer = regularizers.RegularFaceRegularizer()\n",
    "    criterion = losses.ArcFaceLoss(\n",
    "        num_classes=10,\n",
    "        embedding_size=128,\n",
    "        margin=28.6,\n",
    "        scale=64,\n",
    "        weight_regularizer=regularizer,\n",
    "        distance=distance,\n",
    "    )\n",
    "    # GPUが使えるなら使う\n",
    "    if device != \"cpu\":\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    # オプティマイザの設定\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    return model, criterion, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの出力をKNNでラベルに変換する\n",
    "def eval(model, train_loader, test_loader):\n",
    "    # モデルをevalモードにする\n",
    "    model.eval()\n",
    "\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    with torch.no_grad():\n",
    "        # 学習データの推論結果を得る\n",
    "        for x_org, y in tqdm(train_loader, total=len(train_loader)):\n",
    "            # デバイスの指定\n",
    "            x_org, y = x_org.to(device), y.to(device)\n",
    "            # モデルでx_orgを新しい空間に写像\n",
    "            x = model(x_org)\n",
    "            x_train.append(x)\n",
    "            y_train.append(y)\n",
    "        # テストデータの推論結果を得る\n",
    "        for x_org, y in tqdm(test_loader, total=len(test_loader)):\n",
    "            # デバイスの指定\n",
    "            x_org, y = x_org.to(device), y.to(device)\n",
    "            x = model(x_org)\n",
    "            x_test.append(x)\n",
    "            y_test.append(y)\n",
    "    # データを変換する\n",
    "    x_train = torch.cat(x_train).cpu().numpy()\n",
    "    x_test = torch.cat(x_test).cpu().numpy()\n",
    "    y_train = torch.cat(y_train).cpu().numpy()\n",
    "    y_test = torch.cat(y_test).cpu().numpy()\n",
    "\n",
    "    # KNNモデルを作成\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric=\"cosine\")\n",
    "    # KNNモデルを学習データの結果で学習する\n",
    "    knn.fit(x_train, y_train)\n",
    "\n",
    "    # テストデータの推定ラベルをKNNモデルで推論\n",
    "    y_pred = knn.predict(x_test)\n",
    "\n",
    "    return {\n",
    "        \"x_train\": x_train,\n",
    "        \"x_test\": x_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "\n",
    "def run(\n",
    "    pretrained: bool = True,\n",
    "    num_epochs: int = 100,\n",
    "):\n",
    "    # 距離学習\n",
    "    # 事前学習済みのResNetモデルをロード\n",
    "    model, criterion, optimizer = get_model(pretrained=pretrained)\n",
    "\n",
    "    result = []\n",
    "    # 出力先のディレクトリを作成\n",
    "    output_dir = Path(\n",
    "        \"output\", \"metric_learning\", \"pretrained\" if pretrained else \"un_pretrained\"\n",
    "    )\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # 学習\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, output, labels = validate_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "        # KNNで評価する\n",
    "        knn_result = eval(model, train_loader, test_loader)\n",
    "        # ラベルの正解率\n",
    "        label_acc = (knn_result[\"y_pred\"] == knn_result[\"y_test\"]).sum() / len(\n",
    "            knn_result[\"y_test\"]\n",
    "        )\n",
    "        result.append(\n",
    "            {\"train_loss\": train_loss, \"val_loss\": val_loss, \"val_acc\": label_acc}\n",
    "        )\n",
    "        # 結果を保存する\n",
    "        torch.save(\n",
    "            {\"output\": output, \"label\": labels, \"pred_labels\": knn_result[\"y_pred\"]},\n",
    "            output_dir / f\"epoch_{epoch}_output.pt\",\n",
    "        )\n",
    "\n",
    "    df_result = pd.DataFrame(result)\n",
    "    df_result.to_csv(output_dir / \"training_curve.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(pretrained=True, num_epochs=10)\n",
    "# run(train_samples=5000, test_samples=1000, pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの評価\n",
    "学習済みモデルをロードし、テストデータで評価する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_to_array(data_loader):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in data_loader:\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    data = torch.cat(images)\n",
    "    label = torch.cat(labels)\n",
    "\n",
    "    data_reshaped = data.view(data.shape[0], -1).numpy()\n",
    "    return data_reshaped, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## t-SNEによる次元圧縮\n",
    "data_reshaped, label = data_loader_to_array(test_loader)\n",
    "\n",
    "\n",
    "def plot_tsne(data, label):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    data_tsne = tsne.fit_transform(data)\n",
    "    df = pd.DataFrame(data_tsne, columns=[\"x\", \"y\"])\n",
    "    df[\"label\"] = label.numpy().astype(str)\n",
    "    df = df.sort_values(\"label\")\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        title=\"t-SNE Visualization of Image Data\",\n",
    "        color=\"label\",\n",
    "    )\n",
    "    fig.update_layout(legend_title=\"label\")\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tsne(data_reshaped, label)\n",
    "fig.write_image(\"output/metric_learning/tsne_org.png\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = sorted(list(Path(\"output/metric_learning/pretrained\").glob(\"**/epoch_*\")))\n",
    "print(outputs)\n",
    "\n",
    "for i, output in tqdm(enumerate(outputs)):\n",
    "    if i % 10 != 0:\n",
    "        continue\n",
    "    loaded_data = torch.load(output)\n",
    "    output_data = loaded_data[\"output\"]\n",
    "    true_label = loaded_data[\"label\"]\n",
    "\n",
    "    fig = plot_tsne(output_data, true_label)\n",
    "    fig.write_image(f\"output/metric_learning/tsne_epoch_{i}.png\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    pd.read_csv(\"output/metric_learning/pretrained/training_curve.csv\"),\n",
    "    y=[\"train_loss\", \"val_loss\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    pd.read_csv(\"output/metric_learning/pretrained/training_curve.csv\"),\n",
    "    y=[\"val_acc\"],\n",
    ")\n",
    "fig.update_layout(title=\"KNNによるAccuracy\")\n",
    "fig.update_yaxes(title=\"validation Accuracy\")\n",
    "fig.update_xaxes(title=\"epoch\")\n",
    "fig.write_image(\"output/metric_learning/pretrained/accuracy.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
